---
layout: archive
title: "Academic Log"
permalink: /academiclog/
author_profile: true
---

# November 2023

In light of my discussions in the previous month, I made a strategic decision to shift my focus, opting to discontinue exploration of the Neural Tangent Kernel (NTK) of the controlled ResNets. However, I maintained a steadfast commitment to working with signatures in the realm of kernel methods. A significant portion of my efforts was dedicated to deepening my understanding of signature kernels, delving into relevant literature such as [1], [2], and [4].

In the course of my research, I encountered the concepts of universality and characteristicness within the broader scope of kernel learning. Additionally, I came across kernel scoring rules, a discovery facilitated by the insights provided in [5], along with an exploration of the associated concept of strict properness. This led me to engage in the development of a more straightforward proof (or so I believe) regarding the strict properness of the scoring rule outlined in [5].

In parallel, my curiosity extended to pondering the Reproducing Kernel Hilbert Space (RKHS) associated with the signature kernel. This contemplation added a layer of depth to my exploration of kernel methods. Moreover, November proved to be a fruitful month for expanding my knowledge in Analysis within Banach spaces, enriching my understanding of the mathematical foundations underpinning my research pursuits.

### Main works of November:
1. Király, Franz J., and Harald Oberhauser. ‘Kernels for Sequentially Ordered Data’. arXiv, 29 January 2016. [link](http://arxiv.org/abs/1601.08169).
2. Lee, Darrick, and Harald Oberhauser. ‘The Signature Kernel’. arXiv, 8 May 2023. [link](https://doi.org/10.48550/arXiv.2305.04625).
3. Salvi, Cristopher, Thomas Cass, James Foster, Terry Lyons, and Weixin Yang. ‘The Signature Kernel Is the Solution of a Goursat PDE’. SIAM Journal on Mathematics of Data Science 3, no. 3 (January 2021): 873–99. [link](https://doi.org/10.1137/20M1366794).
4. Cass, Thomas, Terry Lyons, and Xingcheng Xu. ‘General Signature Kernels’. arXiv, 1 July 2021. [link](https://doi.org/10.48550/arXiv.2107.00447).
5. Issa, Zacharia, Blanka Horvath, Maud Lemercier, and Cristopher Salvi. ‘Non-Adversarial Training of Neural SDEs with Signature Kernel Scores’. arXiv, 25 May 2023. [link](https://doi.org/10.48550/arXiv.2305.16274).
6. Steinwart, Ingo, and Johanna F. Ziegel. ‘Strictly Proper Kernel Scores and Characteristic Kernels on Compact Spaces’. Applied and Computational Harmonic Analysis 51 (1 March 2021): 510–42. [link](https://doi.org/10.1016/j.acha.2019.11.005).


# October 2023

In the initial part of October, my primary focus was directed towards exploring the applications of path signatures in the context of portfolio optimization [1]. It was during this exploration that I became acquainted with the universality property of signatures, a concept likely to feature prominently in my upcoming thesis. Despite lacking a clear thesis direction, fueled by curiosity, I delved into Stochastic Portfolio Theory and Signature-based methods within this domain, aiming to grasp their implications and potential advantages [2].

However, a significant shift occurred as my attention veered towards kernel learning. The introduction to the Neural Tangent Kernel (NTK) marked a pivotal moment, prompting further exploration into Neural Signature Kernels [3]. In an effort to deepen my comprehension, I undertook the derivation of an explicit expression for the NTK of the controlled ResNets as detailed in [3]. Concurrently, I delved into the tensor programs framework [4,5]. The direct derivation posed challenges with intricate recursions, and the tensor programs raised reservations regarding the arguments presented. Seeking clarification, I reached out to the principal author of [4], learning of progress in deriving an NTK expression, with certain technicalities yet to be resolved. In addition, it's noteworthy to mention that most things related to Deep Learning or Kernel Learning were entirely novel to me, rendering this month particularly enriching in terms of knowledge acquisition.

### Main works of October: 
1. Futter, Owen, Blanka Horvath, and Magnus Wiese. ‘Signature Trading: A Path-Dependent Extension of the Mean-Variance Framework with Exogenous Signals’. SSRN Scholarly Paper. Rochester, NY, 24 August 2023. [link](https://doi.org/10.2139/ssrn.4541830).
2. Cuchiero, Christa, and Janka Möller. ‘Signature Methods in Stochastic Portfolio Theory’. arXiv, 3 October 2023. [link](https://doi.org/10.48550/arXiv.2310.02322).
3. Cirone, Nicola Muca, Maud Lemercier, and Cristopher Salvi. ‘Neural Signature Kernels as Infinite-Width-Depth-Limits of Controlled ResNets’. arXiv, 4 June 2023. [link](https://doi.org/10.48550/arXiv.2303.17671).
4. Yang, Greg. ‘Tensor Programs II: Neural Tangent Kernel for Any Architecture’. arXiv, 29 November 2020. [link](https://doi.org/10.48550/arXiv.2006.14548).
5. Yang, Greg. ‘Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture Are Gaussian Processes’. arXiv, 8 May 2021. [link](https://doi.org/10.48550/arXiv.1910.12478).
6. Karatzas, Ioannis, and Robert Fernholz. ‘Stochastic Portfolio Theory: An Overview’. In Handbook of Numerical Analysis, 15:89–167. Elsevier, 2009. [link](https://doi.org/10.1016/S1570-8659(08)00003-3).





# September 2023

Initiated the process of selecting supervision for my thesis during this month. By the end of September, I confirmed my academic team, consisting of Prof. Fenghui Yu as the primary supervisor and Prof. Christa Cuchiero as the co-supervisor. Despite the lack of a clearly defined thesis direction, I expressed a keen interest in exploring something related to Rough Path Theory, specifically focusing on Path Signatures. The allure of signatures, discovered in the previous semester, stemmed from their integration of both Analysis and Algebra, providing a compelling intersection that resonated with my academic pursuits. And, beyond their mathematical elegance, signatures offer many practical applications in fields such as Machine Learning and Financial Mathematics. Limited time during the month allowed for only light reading, but this did not hinder progress, as I found myself comfortably ahead of schedule. 
